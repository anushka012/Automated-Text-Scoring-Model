{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "question_scoring.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbdZIkEklrgaCJK4JqYJ6b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushka012/Automated-Text-Scoring-Model/blob/main/question_scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUByHNcizf-8"
      },
      "source": [
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ubtG4vD0Twy"
      },
      "source": [
        "# DATASET_DIR = '/content/training_set.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OvmiVR01HfT"
      },
      "source": [
        "# import zipfile\n",
        "\n",
        "# local_zip_file = '/content/training_set.zip'\n",
        "\n",
        "# zip_refs = zipfile.ZipFile(local_zip_file, 'r')\n",
        "\n",
        "# zip_refs.extractall('/content')\n",
        "# zip_refs.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eSdPC6-zkug"
      },
      "source": [
        "X = pd.read_csv(( '/content/training_set_2 (1).tsv'), sep='\\t', encoding='ISO-8859-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdCF8zgz0TQK"
      },
      "source": [
        "y = X['domain1_score']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71EZWq0J3ddc"
      },
      "source": [
        "X = X.drop(columns=['rater1', 'rater2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "X18wyGp7zpEM",
        "outputId": "d8b409f5-321d-49bd-fee3-189cf23f8a0f"
      },
      "source": [
        "X.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>In five years, my goal is to successfully obta...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>My ultimate goal for the next five years is to...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A few of the goals Iâve set for myself over ...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Let me start by saying that Iâm reallyÂ exci...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>From the moment I read the job description for...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>In the next few years I want to get better at ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>Iâve enjoyed managing a direct report in my ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>In my current role, Iâve been able to progre...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>I looked on Google and it seems like there are...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>I never understand why interviewers ask this i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   essay_id  ...  domain1_score\n",
              "0         1  ...              8\n",
              "1         2  ...              7\n",
              "2         3  ...              9\n",
              "3         4  ...              8\n",
              "4         5  ...              6\n",
              "5         6  ...              5\n",
              "6         7  ...              6\n",
              "7         8  ...              8\n",
              "8         9  ...              3\n",
              "9        10  ...              2\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "JpBv2uNfWrdx",
        "outputId": "9eeca569-9dfe-4d43-d6e2-0be6fcc3a510"
      },
      "source": [
        "X.tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>domain1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>76</td>\n",
              "      <td>3</td>\n",
              "      <td>My biggest weakness is probably biting off mor...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>77</td>\n",
              "      <td>3</td>\n",
              "      <td>My biggest weakness is the fact that I freeze ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>78</td>\n",
              "      <td>3</td>\n",
              "      <td>My biggest weakness is probably procrastinatio...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>79</td>\n",
              "      <td>3</td>\n",
              "      <td>I don't have much patience when working with a...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>80</td>\n",
              "      <td>3</td>\n",
              "      <td>I struggle with organization. While it hasn't ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>81</td>\n",
              "      <td>3</td>\n",
              "      <td>I am incredibly self-motivated, and I sometime...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>82</td>\n",
              "      <td>3</td>\n",
              "      <td>Oftentimes, I can be timid when providing cons...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>83</td>\n",
              "      <td>3</td>\n",
              "      <td>My blunt, straightforward nature has allowed m...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>84</td>\n",
              "      <td>3</td>\n",
              "      <td>Public speaking makes me nervous. While I don'...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>85</td>\n",
              "      <td>3</td>\n",
              "      <td>I'm not great at analyzing data or numbers. Ho...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    essay_id  ...  domain1_score\n",
              "72        76  ...              2\n",
              "73        77  ...              0\n",
              "74        78  ...              1\n",
              "75        79  ...              2\n",
              "76        80  ...              3\n",
              "77        81  ...              4\n",
              "78        82  ...              5\n",
              "79        83  ...              5\n",
              "80        84  ...              2\n",
              "81        85  ...              4\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoNq7Cxdzkw2"
      },
      "source": [
        "minimum_scores = [3, 2, 1, 4, 0, 0, 0, 0, 0]\n",
        "maximum_scores = [8, 10, 9, 7, 6, 5, 9, 30, 60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv3UfNVd39iQ"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9xeKOZT39kj"
      },
      "source": [
        "def essay_to_wordlist(essay_v, remove_stopwords):\n",
        "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
        "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
        "    words = essay_v.lower().split()\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return (words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx4bVUGM39oF"
      },
      "source": [
        "def essay_to_sentences(essay_v, remove_stopwords):\n",
        "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leQEb4cS4PlD"
      },
      "source": [
        "def makeFeatureVec(words, model, num_features):\n",
        "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "    num_words = 0.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            num_words += 1\n",
        "            featureVec = np.add(featureVec,model[word])        \n",
        "    featureVec = np.divide(featureVec,num_words)\n",
        "    return featureVec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfuSRkuP4Pnd"
      },
      "source": [
        "def getAvgFeatureVecs(essays, model, num_features):\n",
        "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
        "    counter = 0\n",
        "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
        "    for essay in essays:\n",
        "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return essayFeatureVecs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqYK-anD4PpU",
        "outputId": "2c979c1f-a42a-431e-a806-e95555522613"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOl_6d9G4YNg"
      },
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
        "from keras.models import Sequential, load_model, model_from_config\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd5Scm8Q4YQV"
      },
      "source": [
        "def get_model():\n",
        "    \"\"\"Define the model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(81, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 81], return_sequences=True))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVGPPezD4YS5"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import cohen_kappa_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr9P_bso4r4X"
      },
      "source": [
        "cv = KFold(n_splits = 5, shuffle = True)\n",
        "results = []\n",
        "y_pred_list = []\n",
        "\n",
        "count = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUv2qr1S9Be3",
        "outputId": "15ed5298-2900-49e8-d515-af265bb51b39"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsgzyYR54r97",
        "outputId": "343339ab-216e-4add-f6bd-9f82334c4239"
      },
      "source": [
        "for traincv, testcv in cv.split(X):\n",
        "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
        "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
        "    \n",
        "    train_essays = X_train['essay']\n",
        "    test_essays = X_test['essay']\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    for essay in train_essays:\n",
        "            # Obtaining all sentences from the training essays.\n",
        "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
        "            \n",
        "    # Initializing variables for word2vec model.\n",
        "    num_features = 81\n",
        "    min_word_count = 1\n",
        "    num_workers = 4\n",
        "    context = 10\n",
        "    downsampling = 1e-3\n",
        "\n",
        "    print(\"Training Word2Vec Model...\")\n",
        "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "    model.init_sims(replace=True)\n",
        "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
        "\n",
        "    clean_train_essays = []\n",
        "    \n",
        "    # Generate training and testing data word vectors.\n",
        "    for essay_v in train_essays:\n",
        "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
        "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
        "    \n",
        "    clean_test_essays = []\n",
        "    for essay_v in test_essays:\n",
        "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
        "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
        "    \n",
        "    trainDataVecs = np.array(trainDataVecs)\n",
        "    testDataVecs = np.array(testDataVecs)\n",
        "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
        "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
        "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
        "    \n",
        "    lstm_model = get_model()\n",
        "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=10)\n",
        "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
        "    y_pred = lstm_model.predict(testDataVecs)\n",
        "    \n",
        "    # Save any one of the 5 models.\n",
        "    if count == 5:\n",
        "         lstm_model.save('/content/final_lstm.h5')\n",
        "    \n",
        "    # Round y_pred to the nearest integer.\n",
        "    y_pred = np.around(y_pred)\n",
        "    \n",
        "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
        "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
        "    print(\"Kappa Score: {}\".format(result))\n",
        "    results.append(result)\n",
        "\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------Fold 1--------\n",
            "\n",
            "Training Word2Vec Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 1, 81)             52812     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                37376     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 90,253\n",
            "Trainable params: 90,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 40ms/step - loss: 43.5910 - mae: 5.6303\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 43.3377 - mae: 5.6142\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 43.1703 - mae: 5.6048\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 43.0779 - mae: 5.5990\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 42.9537 - mae: 5.5905\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 42.7777 - mae: 5.5791\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 42.5566 - mae: 5.5657\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 42.3738 - mae: 5.5543\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 42.2062 - mae: 5.5428\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 41.9475 - mae: 5.5281\n",
            "Kappa Score: 0.0\n",
            "\n",
            "--------Fold 2--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 1, 81)             52812     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 64)                37376     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 90,253\n",
            "Trainable params: 90,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 32ms/step - loss: 46.6991 - mae: 5.9071\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 46.4884 - mae: 5.8951\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 46.3767 - mae: 5.8883\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 46.2212 - mae: 5.8798\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 46.1173 - mae: 5.8726\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 45.9126 - mae: 5.8601\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 45.7198 - mae: 5.8490\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 45.5474 - mae: 5.8393\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 45.4682 - mae: 5.8345\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 45.2990 - mae: 5.8258\n",
            "Kappa Score: 0.0\n",
            "\n",
            "--------Fold 3--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_4 (LSTM)                (None, 1, 81)             52812     \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 64)                37376     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 90,253\n",
            "Trainable params: 90,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 39ms/step - loss: 44.5372 - mae: 5.6357\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 44.2733 - mae: 5.6196\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 44.0536 - mae: 5.6047\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 43.8998 - mae: 5.5963\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 43.6884 - mae: 5.5823\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 43.5742 - mae: 5.5758\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 43.3383 - mae: 5.5627\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 43.1627 - mae: 5.5527\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 43.0046 - mae: 5.5407\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 42.7232 - mae: 5.5200\n",
            "Kappa Score: 0.0\n",
            "\n",
            "--------Fold 4--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_6 (LSTM)                (None, 1, 81)             52812     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 64)                37376     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 90,253\n",
            "Trainable params: 90,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 29ms/step - loss: 43.4644 - mae: 5.7423\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 43.2293 - mae: 5.7265\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 42.9860 - mae: 5.7112\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 42.8237 - mae: 5.6997\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 42.5815 - mae: 5.6837\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 42.3797 - mae: 5.6691\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 42.1795 - mae: 5.6557\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 41.9431 - mae: 5.6414\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 41.7198 - mae: 5.6285\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 41.4368 - mae: 5.6064\n",
            "Kappa Score: 0.0\n",
            "\n",
            "--------Fold 5--------\n",
            "\n",
            "Training Word2Vec Model...\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_8 (LSTM)                (None, 1, 81)             52812     \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (None, 64)                37376     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 90,253\n",
            "Trainable params: 90,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 5s 31ms/step - loss: 42.8548 - mae: 5.5603\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 42.6082 - mae: 5.5447\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 42.3488 - mae: 5.5285\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 42.2087 - mae: 5.5192\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 41.9952 - mae: 5.5050\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 41.7321 - mae: 5.4901\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 41.4514 - mae: 5.4701\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 41.2701 - mae: 5.4595\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 40.9852 - mae: 5.4428\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 40.7241 - mae: 5.4239\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7efe2348cef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Kappa Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDQsHt55zkzZ",
        "outputId": "d3859cf1-4119-401b-fc19-0df2c91be727"
      },
      "source": [
        "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Kappa score after a 5-fold cross validation:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixwfsJgaXzJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf13de8-53b3-4b03-b7de-6d06b0f9af26"
      },
      "source": [
        "answer = list(input(\"Enter your answer:-\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your answer:- I’ve always preferred to work in groups and find that my collaborative nature is one of my strongest attributes. On projects that I directed, I work well to inspire diverse team members and work side by side with them to achieve the project goals. In fact, I’ve increased productivity by ten percent over the course of two years. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbE6fZd_IdDE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3LIaYi1IdFX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}